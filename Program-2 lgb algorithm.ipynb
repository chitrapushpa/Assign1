{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ENVIRONMENT SETUP ########\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "##### TRAINING DATA ############\n",
    "\n",
    "train_data = pd.read_csv('D:/AI ML/train_2016_v2.csv', parse_dates =[\"transactiondate\"])\n",
    "# print train_data.shape\n",
    "train_data.head(10)\n",
    "\n",
    "##### PROPERTY DATA ############\n",
    "\n",
    "property_data = pd.read_csv('D:/AI ML/2016DATA.csv')\n",
    "# print property_data.shape\n",
    "property_data.head(10)\n",
    "# print property_data.dtypes\n",
    "\n",
    "##### Visualize and Clean Data #####\n",
    "\n",
    "plt.figure(figsize=(12,20))\n",
    "property_data.drop('parcelid',axis=1).notnull().mean().sort_values(ascending = True).plot(kind = 'barh')\n",
    "plt.title('Percentage of Information by Feature: (2016 Data)')\n",
    "plt.show()\n",
    "property_data.drop('parcelid',axis=1).notnull().mean().sort_values(ascending = False)\n",
    "\n",
    "##### Pools & Hot tubs ##########\n",
    "\n",
    "# \"0 pools\"\n",
    "property_data.poolcnt.fillna(0,inplace = True)\n",
    "# \"0 hot tubs or spas\"\n",
    "property_data.hashottuborspa.fillna(0,inplace = True)\n",
    "# Convert \"True\" to 1\n",
    "property_data.hashottuborspa.replace(to_replace = True, value = 1,inplace = True)\n",
    "# print property_data['hashottuborspa'].value_counts()\n",
    "\n",
    "# Set properties that have a pool but no info on poolsize equal to the median poolsize value.\n",
    "property_data.loc[property_data.poolcnt==1, 'poolsizesum'] = property_data.loc[property_data.poolcnt==1, 'poolsizesum'].fillna(property_data[property_data.poolcnt==1].poolsizesum.median())\n",
    "# \"0 pools\" = \"0 sq ft of pools\"\n",
    "property_data.loc[property_data.poolcnt==0, 'poolsizesum']=0\n",
    "# \"0 pools with a spa/hot tub\"\n",
    "property_data.pooltypeid2.fillna(0,inplace = True)\n",
    "# \"0 pools without a hot tub\"\n",
    "property_data.pooltypeid7.fillna(0,inplace = True)\n",
    "# Drop redundant feature\n",
    "property_data.drop('pooltypeid10', axis=1, inplace=True)\n",
    "property_data.head()\n",
    "\n",
    "##### Fireplace Data ##########\n",
    "\n",
    "# print property_data['fireplaceflag'].value_counts()\n",
    "# If \"fireplaceflag\" is \"True\" and \"fireplacecnt\" is \"NaN\", we will set \"fireplacecnt\" equal to the median value of \"1\".\n",
    "property_data.loc[(property_data['fireplaceflag'] == True) & (property_data['fireplacecnt'].isnull()), ['fireplacecnt']] = 1\n",
    "# If 'fireplacecnt' is \"NaN\", replace with \"0\"\n",
    "property_data.fireplacecnt.fillna(0,inplace = True)\n",
    "# If \"fireplacecnt\" is 1 or larger \"fireplaceflag\" is \"NaN\", we will set \"fireplaceflag\" to \"True\".\n",
    "property_data.loc[(property_data['fireplacecnt'] >= 1.0) & (property_data['fireplaceflag'].isnull()), ['fireplaceflag']] = True\n",
    "property_data.fireplaceflag.fillna(0,inplace = True)\n",
    "# print property_data['fireplaceflag'].value_counts()\n",
    "# Convert \"True\" to 1\n",
    "property_data.fireplaceflag.replace(to_replace = True, value = 1,inplace = True)\n",
    "# print property_data['fireplaceflag'].value_counts()\n",
    "\n",
    "##### Garage Data ##########\n",
    "\n",
    "property_data.garagecarcnt.fillna(0,inplace = True)\n",
    "property_data.garagetotalsqft.fillna(0,inplace = True)\n",
    "\n",
    "##### Tax Data Delinquency ##########\n",
    "\n",
    "#print property_data['taxdelinquencyflag'].value_counts()\n",
    "# Replace \"NaN\" with \"0\"\n",
    "property_data.taxdelinquencyflag.fillna(0,inplace = True)\n",
    "# Change \"Y\" to \"1\"\n",
    "property_data.taxdelinquencyflag.replace(to_replace = 'Y', value = 1,inplace = True)\n",
    "# print property_data['taxdelinquencyflag'].value_counts()\n",
    "# Drop \"taxdelinquencyyear\"\n",
    "property_data.drop('taxdelinquencyyear', axis=1, inplace=True)\n",
    "\n",
    "##### The Rest ##########\n",
    "\n",
    "# Drop \"storytypeid\"\n",
    "property_data.drop('storytypeid', axis=1, inplace=True)\n",
    "# Replace \"NaN\" with 0, signifying no basement.\n",
    "property_data.basementsqft.fillna(0,inplace = True)\n",
    "#print(property_data['yardbuildingsqft26'].value_counts())\n",
    "# Replace 'yardbuildingsqft26' \"NaN\"s with \"0\".\n",
    "property_data.yardbuildingsqft26.fillna(0,inplace = True)\n",
    "# Drop \"architecturalstyletypeid\"\n",
    "property_data.drop('architecturalstyletypeid', axis=1, inplace=True)\n",
    "# Drop \"typeconstructiontypeid\" and \"finishedsquarefeet13\"\n",
    "property_data.drop('typeconstructiontypeid', axis=1, inplace=True)\n",
    "property_data.drop('finishedsquarefeet13', axis=1, inplace=True)\n",
    "# Drop \"buildingclasstypeid\"\n",
    "property_data.drop('buildingclasstypeid', axis=1, inplace=True)\n",
    "#print property_data.shape\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "# Let's check the unique values for \"decktypeid\"\n",
    "# print(property_data['decktypeid'].value_counts())\n",
    "# Change \"decktypeid\" \"Nan\"s to \"0\"\n",
    "property_data.decktypeid.fillna(0,inplace = True)\n",
    "# Convert \"decktypeid\" \"66.0\" to \"1\"\n",
    "property_data.decktypeid.replace(to_replace = 66.0, value = 1,inplace = True)\n",
    "# print(property_data['decktypeid'].value_counts())\n",
    "# print(property_data['finishedsquarefeet6'].value_counts())\n",
    "\n",
    "#squarefeet = property_data[property_data['finishedsquarefeet6'].notnull() & property_data['finishedsquarefeet12'].isnull() & property_data['finishedsquarefeet15'].isnull() & property_data['finishedsquarefeet50'].isnull() & property_data['lotsizesquarefeet'].isnull()]\n",
    "#squarefeet = property_data[property_data['finishedsquarefeet12'].notnull() & property_data['finishedsquarefeet15'].notnull() & property_data['finishedsquarefeet50'].notnull() & property_data['lotsizesquarefeet'].notnull()]\n",
    "squarefeet = property_data[property_data['finishedsquarefeet15'].notnull() & property_data['finishedsquarefeet50'].notnull() & property_data['lotsizesquarefeet'].notnull()]\n",
    "squarefeet[['calculatedfinishedsquarefeet','finishedsquarefeet6','finishedsquarefeet12','finishedsquarefeet15','finishedsquarefeet50','numberofstories','lotsizesquarefeet','landtaxvaluedollarcnt','structuretaxvaluedollarcnt','taxvaluedollarcnt','taxamount']]\n",
    "#squarefeet\n",
    "# squarefeet = property_data[property_data[['finishedsquarefeet6','finishedsquarefeet12','finishedsquarefeet15','finishedsquarefeet50','lotsizesquarefeet']].notnull()]\n",
    "#property_data[['finishedsquarefeet6','finishedsquarefeet12','finishedsquarefeet15','finishedsquarefeet50','lotsizesquarefeet']][:100]\n",
    "\n",
    "# Drop \"finishedsquarefeet6\"\n",
    "property_data.drop('finishedsquarefeet6', axis=1, inplace=True)\n",
    "# Drop \"finishedsquarefeet12\"\n",
    "property_data.drop('finishedsquarefeet12', axis=1, inplace=True)\n",
    "# Drop \"finishedfloor1squarefeet\"\n",
    "property_data.drop('finishedfloor1squarefeet', axis=1, inplace=True)\n",
    "\n",
    "squarefeet2 = property_data[property_data['finishedsquarefeet15'].notnull() & property_data['finishedsquarefeet50'].notnull() & property_data['lotsizesquarefeet'].notnull()]\n",
    "#squarefeet2 = property_data[property_data['finishedsquarefeet15'].notnull() & property_data['calculatedfinishedsquarefeet'].isnull()]\n",
    "squarefeet2[['calculatedfinishedsquarefeet','finishedsquarefeet15','finishedsquarefeet50','numberofstories','lotsizesquarefeet']]\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "\n",
    "# Replace \"NaN\" \"calculatedfinishedsquarefeet\" values with mean.\n",
    "property_data['calculatedfinishedsquarefeet'].fillna((property_data['calculatedfinishedsquarefeet'].mean()), inplace=True)\n",
    "\n",
    "# Replace \"NaN\" \"finishedsquarefeet15\" values with calculatedfinishedsquarefeet.\n",
    "property_data.loc[property_data['finishedsquarefeet15'].isnull(),'finishedsquarefeet15'] = property_data['calculatedfinishedsquarefeet']\n",
    "#property_data['finishedsquarefeet15'].fillna(property_data['calculatedfinishedsquarefeet'])\n",
    "\n",
    "property_data.numberofstories.fillna(1,inplace = True)\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "# print(property_data['numberofstories'].value_counts())\n",
    "\n",
    "# If \"numberofstories\" is equal to \"1\", then we can replace the \"NaN\"s with the \"calculatedfinishedsquarefeet\" value. Fill in the rest with the average values.\n",
    "property_data.loc[property_data['numberofstories'] == 1.0,'finishedsquarefeet50'] = property_data['calculatedfinishedsquarefeet']\n",
    "property_data['finishedsquarefeet50'].fillna((property_data['finishedsquarefeet50'].mean()), inplace=True)\n",
    "\n",
    "# print property_data.shape\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "\n",
    "# Replace 'yardbuildingsqft17' \"NaN\"s with \"0\".\n",
    "property_data.yardbuildingsqft17.fillna(0,inplace = True)\n",
    "\n",
    "bathrooms = property_data[property_data['fullbathcnt'].notnull() & property_data['threequarterbathnbr'].notnull() & property_data['calculatedbathnbr'].notnull()]\n",
    "bathrooms[['fullbathcnt','threequarterbathnbr','calculatedbathnbr']]\n",
    "\n",
    "# Drop \"threequarterbathnbr\"\n",
    "property_data.drop('threequarterbathnbr', axis=1, inplace=True)\n",
    "# Drop \"fullbathcnt\"\n",
    "property_data.drop('fullbathcnt', axis=1, inplace=True)\n",
    "# Fill in \"NaN\" \"calculatedbathnbr\" with most common\n",
    "bathroommode = property_data['calculatedbathnbr'].value_counts().idxmax()\n",
    "property_data['calculatedbathnbr'] = property_data['calculatedbathnbr'].fillna(bathroommode)\n",
    "\n",
    "# print property_data.shape\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "property_data.airconditioningtypeid.fillna(5,inplace = True)\n",
    "\n",
    "# Drop \"regionidneighborhood\"\n",
    "property_data.drop('regionidneighborhood', axis=1, inplace=True)\n",
    "property_data.heatingorsystemtypeid.fillna(13,inplace = True)\n",
    "# print(property_data['buildingqualitytypeid'].value_counts())\n",
    "\n",
    "# Fill in \"NaN\" \"buildingqualitytypeid\" with most common\n",
    "buildingqual = property_data['buildingqualitytypeid'].value_counts().idxmax()\n",
    "property_data['buildingqualitytypeid'] = property_data['buildingqualitytypeid'].fillna(buildingqual)\n",
    "\n",
    "property_data.unitcnt.fillna(1,inplace = True)\n",
    "# print(property_data['propertyzoningdesc'].value_counts())\n",
    "\n",
    "# Fill in \"NaN\" \"propertyzoningdesc\" with most common\n",
    "propertyzoningdesc = property_data['propertyzoningdesc'].value_counts().idxmax()\n",
    "property_data['propertyzoningdesc'] = property_data['propertyzoningdesc'].fillna(propertyzoningdesc)\n",
    "property_data['lotsizesquarefeet'].fillna((property_data['lotsizesquarefeet'].mean()), inplace=True)\n",
    "\n",
    "# print(property_data['censustractandblock'].value_counts())\n",
    "# print(property_data['rawcensustractandblock'].value_counts())\n",
    "\n",
    "# Drop \"censustractandblock\"\n",
    "property_data.drop('censustractandblock', axis=1, inplace=True)\n",
    "property_data.landtaxvaluedollarcnt.fillna(0,inplace = True)\n",
    "property_data.structuretaxvaluedollarcnt.fillna(0,inplace = True)\n",
    "property_data['taxvaluedollarcnt'].fillna((property_data['taxvaluedollarcnt'].mean()), inplace=True)\n",
    "\n",
    "property_data['taxpercentage'] = property_data['taxamount'] / property_data['taxvaluedollarcnt']\n",
    "property_data.head()\n",
    "property_data['taxpercentage'].fillna((property_data['taxpercentage'].mean()), inplace=True)\n",
    "\n",
    "# Drop \"taxamount\"\n",
    "property_data.drop('taxamount', axis=1, inplace=True)\n",
    "# Drop \"regionidcity\"\n",
    "property_data.drop('regionidcity', axis=1, inplace=True)\n",
    "\n",
    "# Fill in \"NaN\" \"yearbuilt\" with most common\n",
    "yearbuilt = property_data['yearbuilt'].value_counts().idxmax()\n",
    "property_data['yearbuilt'] = property_data['yearbuilt'].fillna(yearbuilt)\n",
    "\n",
    "\n",
    "#print property_data.shape\n",
    "#print property_data.dtypes\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "\n",
    "# Fill in \"fips\" \"NaN\"s\n",
    "fips = property_data['fips'].value_counts().idxmax()\n",
    "property_data['fips'] = property_data['fips'].fillna(fips)\n",
    "\n",
    "# Fill in \"propertylandusetypeid\" \"NaN\"s\n",
    "propertylandusetypeid = property_data['propertylandusetypeid'].value_counts().idxmax()\n",
    "property_data['propertylandusetypeid'] = property_data['propertylandusetypeid'].fillna(propertylandusetypeid)\n",
    "\n",
    "# Drop 'regionidcounty'\n",
    "property_data.drop('regionidcounty', axis=1, inplace=True)\n",
    "\n",
    "# Fill in \"latitude\" \"NaN\"s\n",
    "latitude = property_data['latitude'].value_counts().idxmax()\n",
    "property_data['latitude'] = property_data['latitude'].fillna(latitude)\n",
    "\n",
    "# Fill in \"longitude\" \"NaN\"s\n",
    "longitude = property_data['longitude'].value_counts().idxmax()\n",
    "property_data['longitude'] = property_data['longitude'].fillna(longitude)\n",
    "\n",
    "\n",
    "# Fill in \"rawcensustractandblock\" \"NaN\"s\n",
    "rawcensustractandblock = property_data['rawcensustractandblock'].value_counts().idxmax()\n",
    "property_data['rawcensustractandblock'] = property_data['rawcensustractandblock'].fillna(rawcensustractandblock)\n",
    "\n",
    "# Fill in \"assessmentyear\" \"NaN\"s\n",
    "assessmentyear = property_data['assessmentyear'].value_counts().idxmax()\n",
    "property_data['assessmentyear'] = property_data['assessmentyear'].fillna(assessmentyear)\n",
    "\n",
    "# Fill in \"bedroomcnt\" \"NaN\"s\n",
    "bedroomcnt = property_data['bedroomcnt'].value_counts().idxmax()\n",
    "property_data['bedroomcnt'] = property_data['bedroomcnt'].fillna(bedroomcnt)\n",
    "\n",
    "# Fill in \"bathroomcnt\" \"NaN\"s\n",
    "bathroomcnt = property_data['bathroomcnt'].value_counts().idxmax()\n",
    "property_data['bathroomcnt'] = property_data['bathroomcnt'].fillna(bathroomcnt)\n",
    "\n",
    "# Fill in \"roomcnt\" \"NaN\"s\n",
    "roomcnt = property_data['roomcnt'].value_counts().idxmax()\n",
    "property_data['roomcnt'] = property_data['roomcnt'].fillna(roomcnt)\n",
    "\n",
    "# Fill in \"propertycountylandusecode\" \"NaN\"s\n",
    "propertycountylandusecode = property_data['propertycountylandusecode'].value_counts().idxmax()\n",
    "property_data['propertycountylandusecode'] = property_data['propertycountylandusecode'].fillna(propertycountylandusecode)\n",
    "\n",
    "# Fill in \"regionidzip \" \"NaN\"s\n",
    "regionidzip = property_data['regionidzip'].value_counts().idxmax()\n",
    "property_data['regionidzip'] = property_data['regionidzip'].fillna(regionidzip)\n",
    "\n",
    "#print property_data.shape\n",
    "property_data.notnull().mean().sort_values(ascending = False)\n",
    "\n",
    "###### LOG ERROR ###########\n",
    "\n",
    "train_with_months = train_data\n",
    "train_with_months['sale_month'] = train_with_months['transactiondate'].apply(lambda x: (x.to_pydatetime()).month)\n",
    "train_with_months['sale_day'] = train_with_months['transactiondate'].apply(lambda x: (x.to_pydatetime()).day)\n",
    "train_with_months['sale_year'] = train_with_months['transactiondate'].apply(lambda x: (x.to_pydatetime()).year)\n",
    "train_with_months.head(10)\n",
    "\n",
    "######### SALES PER MONTH #########\n",
    "\n",
    "color = sns.color_palette()\n",
    "num_of_sales = train_with_months['sale_month'].value_counts()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(num_of_sales.index, num_of_sales.values, alpha=0.8, color=color[5])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Sale Month', fontsize=12)\n",
    "plt.ylabel('Number of Sales', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "######### NORMAL LOG ERROR #########\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.distplot(train_with_months.logerror.values, bins=500, kde=False)\n",
    "plt.xlabel('Log Error')\n",
    "mu, std = norm.fit(train_with_months.logerror.values)\n",
    "xmin,xmax=plt.xlim()\n",
    "x = np.linspace(-5,5,1000)\n",
    "p = norm.pdf(x, 0, 0.025)\n",
    "plt.plot(x, p*(x[1]-x[0])*90275, 'k', linewidth=2)\n",
    "plt.axis([-0.3,0.3,0,20000])\n",
    "plt.ylabel('Normal')\n",
    "plt.show()\n",
    "\n",
    "######### AVERAGE LOG ERROR PER MONTH #########\n",
    "\n",
    "avg_logs_table = []\n",
    "for x in range(1,13):\n",
    "    avg_logs = np.mean(train_with_months[train_with_months['sale_month'] == x]['logerror'])\n",
    "    avg_logs_table.append(avg_logs)\n",
    "\n",
    "t = range(1,13)\n",
    "plt.plot(t,avg_logs_table)\n",
    "plt.xticks(np.linspace(0,11,12,endpoint=True))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Log Error')\n",
    "plt.show()\n",
    "\n",
    "######### AVERAGE ABSOLUTE LOG ERROR PER MONTH #########\n",
    "\n",
    "abs_log_error_list = train_with_months[['logerror','sale_month']]\n",
    "abs_log_error_list['logerror'] = np.abs(abs_log_error_list['logerror'])\n",
    "\n",
    "avg_abs_logs_table = []\n",
    "for i in range(1,13):\n",
    "    avg_logs = np.mean(abs_log_error_list[abs_log_error_list['sale_month'] == i]['logerror'])\n",
    "    avg_abs_logs_table.append(avg_logs)\n",
    "\n",
    "plt.plot(avg_abs_logs_table)\n",
    "plt.xticks(np.linspace(0,11,12,endpoint=True))\n",
    "plt.yticks(np.linspace(0.05,0.09,5,endpoint=True))\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Absolute Log Error')\n",
    "plt.show()\n",
    "\n",
    "######### Create New Features #########\n",
    "\n",
    "property_data['taxpersqft'] = property_data['taxvaluedollarcnt'] / property_data['calculatedfinishedsquarefeet']\n",
    "property_data['bathpersqft'] = property_data['bathroomcnt'] / property_data['calculatedfinishedsquarefeet']\n",
    "property_data['roompersqft'] = property_data['roomcnt'] / property_data['calculatedfinishedsquarefeet']\n",
    "property_data['bedroompersqft'] = property_data['bedroomcnt'] / property_data['calculatedfinishedsquarefeet']\n",
    "\n",
    "######### Merge the Datasets #########\n",
    "\n",
    "#train_data.drop(['sale_year','sale_day'],axis=1,inplace=True)\n",
    "merged_data = train_data.merge(property_data,on='parcelid',how='left')\n",
    "merged_data.head(5)\n",
    "\n",
    "train_data.drop(['sale_month','sale_year','sale_day'],axis=1,inplace=True)\n",
    "\n",
    "######### Draw Geographical Maps #########\n",
    "\n",
    "# exploratory analysis\n",
    "# location distribution\n",
    "plt.figure(figsize=(12,12))\n",
    "fig=sns.jointplot(x=merged_data.longitude.values, y=merged_data.latitude.values, size=10,color='b')\n",
    "fig.set_axis_labels('Longitude','Latitude')\n",
    "#plt.colorbar(orientation=\"vertical\",fraction=0.07)\n",
    "plt.show()\n",
    "\n",
    "######### Univariate Analysis #########\n",
    "\n",
    "# Now let us look at the correlation coefficient of each of these variables #\n",
    "x_cols = [col for col in merged_data.columns if col not in ['logerror'] if merged_data[col].dtype == 'float64']\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "for col in x_cols:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(merged_data[col].values, merged_data.logerror.values)[0, 1])\n",
    "corr_df = pd.DataFrame({'col_labels': labels, 'corr_values': values})\n",
    "corr_df = corr_df.sort_values(by='corr_values')\n",
    "\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12, 40))\n",
    "rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='b')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation coefficient of the variables\")\n",
    "# autolabel(rects)\n",
    "plt.show()\n",
    "\n",
    "corr_df_sel = corr_df.ix[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\n",
    "\n",
    "######### Important variables correlation map #########\n",
    "\n",
    "cols_to_use = corr_df_sel.col_labels.tolist()\n",
    "temp_df = merged_data[cols_to_use]\n",
    "corrmat = temp_df.corr(method='spearman')\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(corrmat, vmax=1., square=True)\n",
    "plt.title(\"Important variables correlation map\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "import gc\n",
    "\n",
    "#print('Loading data ...')\n",
    "#train = pd.read_csv('../input/train_2016.csv')\n",
    "#prop = pd.read_csv('../input/properties_2016.csv')\n",
    "\n",
    "for c, dtype in zip(merged_data.columns, merged_data.dtypes):\t\n",
    "    if dtype == np.float64 or dtype == np.int64:\t\t\n",
    "        merged_data[c] = merged_data[c].astype(np.float32)\n",
    "\n",
    "#df_train = train_data.merge(property_data, how='left', on='parcelid')\n",
    "df_train = merged_data\n",
    "\n",
    "\n",
    "merged_data.drop(['sale_month','sale_year','sale_day'],axis=1,inplace=True)\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "split = 90000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "x_valid = x_valid.values.astype(np.float32, copy=False)\n",
    "\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "\n",
    "params = {}\n",
    "params['learning_rate'] = 0.002\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'mae'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 60\n",
    "params['min_data'] = 500\n",
    "params['min_hessian'] = 1\n",
    "\n",
    "'''\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "'''\n",
    "\n",
    "watchlist = [d_valid]\n",
    "clf = lgb.train(params, d_train, 500, watchlist)\n",
    "\n",
    "del d_train, d_valid; gc.collect()\n",
    "del x_train, x_valid; gc.collect()\n",
    "\n",
    "print(\"Prepare for the prediction ...\")\n",
    "sample = pd.read_csv('D:/AI ML/sample_submission.csv')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "df_test = sample.merge(property_data, on='parcelid', how='left')\n",
    "del sample; gc.collect()\n",
    "x_test = df_test[train_columns]\n",
    "del df_test; gc.collect()\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "print(\"Start prediction ...\")\n",
    "# num_threads > 1 will predict very slow in kernal\n",
    "clf.reset_parameter({\"num_threads\":1})\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print(\"Start write result ...\")\n",
    "sub = pd.read_csv('D:/AI ML/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test\n",
    "\n",
    "sub.to_csv('D:/AI ML/lgb_results.csv', index=False, float_format='%.4f')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
